1
00:00:00,390 --> 00:00:03,860
In an earlier video, I've written down a form for the cost function for
在前面的视频中 我已经分析了逻辑回归的代价函数

2
00:00:03,860 --> 00:00:05,230
logistical regression.
的表达式

3
00:00:05,230 --> 00:00:09,370
In this optional video, I want to give you a quick justification for
在这节选修视频中 我将给出一个简洁的证明

4
00:00:09,370 --> 00:00:13,490
why we like to use that cost function for logistic regression.
来说明逻辑回归代价函数的表达式为什么是这种形式

5
00:00:13,490 --> 00:00:17,709
To quickly recap, in logistic regression,
回想一下 在逻辑回归中

6
00:00:17,709 --> 00:00:23,704
we have that the prediction y hat is sigmoid of w transpose x + b,
需要预测的结果y~ 可以表示为sigmod(w^T*x + b)

7
00:00:23,704 --> 00:00:27,711
where sigmoid is this familiar function.
sigmoid是我们熟悉的S型函数sigmoid(z)=1/(1+e^-z)

8
00:00:27,711 --> 00:00:34,353
And we said that we want to interpret y hat as the p( y = 1 | x).
我们约定 y~=p(y = 1 | x)

9
00:00:34,353 --> 00:00:39,776
So we want our algorithm to output y hat as the chance
y~=p(y = 1 | x) 即算法的输出y~

10
00:00:39,776 --> 00:00:45,030
that y = 1 for a given set of input features x.
是给定训练样本x条件下 y 等于1的概率

11
00:00:45,030 --> 00:00:50,090
So another way to say this is that if y is equal to 1
换句话说 如果y=1

12
00:00:50,090 --> 00:00:56,020
then the chance of y given x is equal to y hat.
在给定训练样本x条件下 y等于y~

13
00:00:56,020 --> 00:00:59,170
And conversely if y is equal to 0 then
反过来说 如果y=0

14
00:01:00,310 --> 00:01:05,840
the chance that y was 0 was 1- y hat, right?
在给定训练样本x条件下 y等于1减去y~(y=1-y~)

15
00:01:05,840 --> 00:01:09,150
So if y hat was a chance, that y = 1,
因此 如果y~ 代表y=1的概率

16
00:01:09,150 --> 00:01:13,620
then 1-y hat is the chance that y = 0.
那么1-y~ 就是y=0的概率

17
00:01:13,620 --> 00:01:18,057
So, let me take these last two equations and just copy them to the next slide.
接下来 我们就来分析这两个条件概率公式

18
00:01:18,057 --> 00:01:22,684
So what I'm going to do is take these two equations which
这两个条件概率公式定义

19
00:01:22,684 --> 00:01:28,010
basically define p(y|x) for the two cases of y = 0 or y = 1.
形式为p(y|x) 并且代表了 y=0 或者 y=1 这两种情况

20
00:01:28,010 --> 00:01:33,110
And then take these two equations and summarize them into a single equation.
我们可以将这两个公式合并成一个公式

21
00:01:33,110 --> 00:01:37,543
And just to point out y has to be either 0 or 1 because when binary cost equations,
需要指出的是我们讨论的是二分类问题的代价函数

22
00:01:37,543 --> 00:01:41,110
so y = 0 or 1 are the only two possible cases, all right.
因此 y的取值只能是 0 或者 1

23
00:01:41,110 --> 00:01:44,653
When someone take these two equations and summarize them as follows.
上述的两个条件概率公式可以合并成 p(y|x)=(y~)^y*[1-(y~)]^(1-y)

24
00:01:44,653 --> 00:01:48,774
Let me just write out what it looks like, then we'll explain why it looks like that.
接下来 我会解释为什么可以合并成 这种形式的表达式

25
00:01:48,774 --> 00:01:54,040
So (1 – y hat) to the power of (1 – y).
(1-y~) 的 (1-y) 次方

26
00:01:54,040 --> 00:01:58,920
So it turns out this one line summarizes the two equations on top.
因此这行表达式包含了上面的两个条件概率公式

27
00:01:58,920 --> 00:02:00,500
Let me explain why.
我来解释一下为什么这行包含了上面两个条件概率公式

28
00:02:00,500 --> 00:02:04,643
So in the first case, suppose y = 1, right?
第一种情况 假设 y=1

29
00:02:04,643 --> 00:02:09,562
So if y = 1 then this term ends up being y hat,
由于y=1 那么(y~)^y = y~

30
00:02:09,562 --> 00:02:13,970
because that's y hat to the power of 1.
因为 y~ 的 1 次方 等于 y~

31
00:02:13,970 --> 00:02:21,120
This term ends up being 1- y hat to the power of 1- 1, so that's the power of 0.
(1-y~)^(1-y) 的 指数项(1-y)等于0

32
00:02:21,120 --> 00:02:26,320
But, anything to the power of 0 is equal to 1, so that goes away.
由于任何数的0次方都是1 y~乘以1 等于y~

33
00:02:26,320 --> 00:02:33,030
And so, this equation, just as p(y|x) = y hat, when y = 1.
因此当y=1时 p(y|x)=y~

34
00:02:33,030 --> 00:02:37,480
So that's exactly what we wanted.
这就是最终的结果

35
00:02:37,480 --> 00:02:40,250
Now how about the second case, what if y = 0?
第二种情况 当y=0时 p(y|x)等于多少呢?

36
00:02:40,250 --> 00:02:47,057
If y = 0, then this equation above is p(y|x) = y hat to the 0,
假设y=0 y~的y次方 就是 y~的0次方

37
00:02:47,057 --> 00:02:51,920
but anything to the power of 0 is equal to 1, so
任何数的0次方都等于1

38
00:02:51,920 --> 00:02:58,267
that's just equal to 1 times 1- y hat to the power of 1- y.
因此 p(y|x)=1*(1-y~)^(1-y)

39
00:02:58,267 --> 00:03:02,770
So 1- y is 1- 0, so this is just 1.
前面假设y=0 因此1-y就等于1

40
00:03:02,770 --> 00:03:07,610
And so this is equal to 1 times (1- y hat) = 1- y hat.
因此p(y|x) = 1*(1-y~) = 1-y~

41
00:03:10,700 --> 00:03:17,230
And so here we have that the y = 0,p(y|x) = 1- y hat,
因此在这里当y=0时 p(y|x)=1-y~

42
00:03:17,230 --> 00:03:21,570
which is exactly what we wanted above.
这就是这个公式(第二个公式)的结果

43
00:03:21,570 --> 00:03:23,690
So what we've just shown is that this equation
因此 刚才的推导表明p(y|x)=(y~)^y * (1-y~)^(1-y)

44
00:03:25,330 --> 00:03:30,331
is a correct definition for p(ylx).
就是p(y|x)的完整定义

45
00:03:30,331 --> 00:03:36,513
Now, finally because the log function is a strictly monotonically
由于 log 函数是严格单调递增的函数

46
00:03:36,513 --> 00:03:43,223
increasing function, you're maximizing log(p(y|x)) give you similar result
最大化 log(p(y|x)) 等价于

47
00:03:43,223 --> 00:03:48,672
that is optimizing p(y|x) and if you compute log of p(y|x),
最大化 p(y|x) 并且地计算 p(y|x)的 log对数

48
00:03:48,672 --> 00:03:54,330
that's equal to log of y hat r of y 1- y at sub par of 1- y.
就是计算 log((y~)^y * (1-y~)^(1-y)) (其实就是将p(y|x)代入)

49
00:03:54,330 --> 00:03:59,818
And so that simplifies to y log y hat
通过对数函数化简为

50
00:03:59,818 --> 00:04:05,881
+ 1-y times log 1- y hat, right?
y*logy~ + (1-y)*log(1-y~)

51
00:04:05,881 --> 00:04:09,832
And so this is actually negative of the loss
而这就是我们前面提到的

52
00:04:09,832 --> 00:04:14,310
function that we had to find previously.
代价函数的负数(-L(y~,y))

53
00:04:14,310 --> 00:04:17,470
And there's a negative sign there because usually if you're training a learning
前面有一个负号的原因是 当你训练学习算法时

54
00:04:17,470 --> 00:04:20,460
algorithm, you want to make probabilities large
需要算法输出值的概率是最大的（以最大的概率 预测这个值）

55
00:04:20,460 --> 00:04:23,980
whereas in logistic regression we're expressing this.
然而在逻辑回归中

56
00:04:23,980 --> 00:04:25,820
We want to minimize the loss function.
我们需要最小化损失函数

57
00:04:25,820 --> 00:04:30,640
So minimizing the loss corresponds to maximizing the log of the probability.
因此最小化损失函数 与 最大化条件概率的对数（log(p(y|x))）关联起来了

58
00:04:30,640 --> 00:04:33,925
So this is what the loss function on a single example looks like.
因此这就是单个训练样本的损失函数表达式

59
00:04:33,925 --> 00:04:35,435
How about the cost function,
这个损失函数

60
00:04:35,435 --> 00:04:40,435
the overall cost function on the entire training set on m examples?
在m个训练样本的整个训练集中又该如何表示呢?

61
00:04:40,435 --> 00:04:41,385
Let's figure that out.
让我们一起来探讨一下

62
00:04:41,385 --> 00:04:45,710
So, the probability of all the labels In the training set.
整个训练集中的样本值(label 要么是0，要么是1) 的概率

63
00:04:45,710 --> 00:04:47,750
Writing this a little bit informally.
更正式地

64
00:04:47,750 --> 00:04:51,945
If you assume that the training examples I've drawn independently or drawn IID,
假设所有的训练样本服从同一分布且相互独立

65
00:04:51,945 --> 00:04:54,198
identically independently distributed,
也即 独立同分布的

66
00:04:54,198 --> 00:04:57,810
then the probability of the example is the product of probabilities.
所有这些样本的联合概率就是每个样本概率的乘积

67
00:04:57,810 --> 00:05:03,143
The product from i = 1 through m p(y(i) ) given x(i).
从1到m的 给定每个训练样本x^i条件下 y^(i) 的概率乘积

68
00:05:03,143 --> 00:05:07,970
And so if you want to carry out maximum likelihood estimation, right,
要想求解 概率累积(m个样本的概率乘积) 的最大似然估计

69
00:05:07,970 --> 00:05:12,476
then you want to maximize the, find the parameters that maximizes
需要寻找一组参数 使得在训练集中

70
00:05:12,476 --> 00:05:15,948
the chance of your observations in the training set.
给出的样本出现的概率最大

71
00:05:15,948 --> 00:05:20,200
But maximizing this is the same as maximizing the log, so
由于最大化 概率累积 等价于 最大化 概率累积的对数

72
00:05:20,200 --> 00:05:22,990
we just put logs on both sides.
在等式两边取对数

73
00:05:22,990 --> 00:05:28,640
So log of the probability of the labels in the training set is equal to,
训练集中的样本值出现的概率的对数 等于

74
00:05:28,640 --> 00:05:30,990
log of a product is the sum of the log.
对 每个样本的条件概率p(y^i|x^i)的对数 求和

75
00:05:30,990 --> 00:05:39,000
So that's sum from i=1 through m of log p(y(i)) given x(i).
也即从1到m对 log(p(y^i|x^i)) 求和

76
00:05:39,000 --> 00:05:43,582
And we have previously figured out on the previous
在前面视频中我们讲过

77
00:05:43,582 --> 00:05:47,630
slide that this is negative L of y hat i, y i.
它就等于 -L((y~)^i, y^i)

78
00:05:48,850 --> 00:05:55,220
And so in statistics, there's a principle called the principle of maximum likelihood
在统计学里面 有一个方法叫做最大似然估计

79
00:05:55,220 --> 00:06:00,720
estimation, which just means to choose the parameters that maximizes this thing.
即 求出一组参数 使得表达式 取最大值

80
00:06:00,720 --> 00:06:04,220
Or in other words, that maximizes this thing.
或者说 使似然函数取最大值

81
00:06:04,220 --> 00:06:09,510
Negative sum from i = 1 through m L(y hat ,y) and
负1乘以从 1到m 对L((y~)^i, y^i)求和

82
00:06:09,510 --> 00:06:11,840
just move the negative sign outside the summation.
可以将负号移到求和符号的外面

83
00:06:11,840 --> 00:06:15,749
So this justifies the cost we had for
至此 已经验证了前面给出的逻辑回归的代价函数J(w,b)

84
00:06:15,749 --> 00:06:21,235
logistic regression which is J(w,b) of this.
与这里 得到 的形式是一致的(这里从概率的角度解释了代价函数：
最大化训练样本集出现的概率就相当于最小化代价函数)

85
00:06:21,235 --> 00:06:27,349
And because we now want to minimize the cost instead of maximizing likelihood,
由于训练模型时 是求解最小化代价函数 而不是最大化似然函数

86
00:06:27,349 --> 00:06:30,095
we've got to rid of the minus sign.
因此去掉这里的负号

87
00:06:30,095 --> 00:06:35,467
And then finally for convenience, we make sure that our quantities are better scale,
最终为了化简代价函数 可以对代价函数进行适当的缩放

88
00:06:35,467 --> 00:06:39,310
we just add a 1 over m extra scaling factor there.
在前面乘以 (1/m) 的一个常数因子

89
00:06:39,310 --> 00:06:43,960
But so to summarize, by minimizing this cost function J(w,b) we're really
总结一下 为了最小化代价函数J(w,b) 我们

90
00:06:43,960 --> 00:06:48,430
carrying out maximum likelihood estimation with the logistic regression model.
从逻辑回归模型的最大似然估计的角度出发

91
00:06:48,430 --> 00:06:53,120
Under the assumption that our training examples were IID, or
在训练集中的样本都是独立同分布的条件下，将

92
00:06:53,120 --> 00:06:55,530
identically independently distributed.
(将最大似然估计 与 代价函数等价起来)

93
00:06:55,530 --> 00:06:59,550
So thank you for watching this video, even though this is optional.
尽管这节课是选修性质的 但还是感谢观看本节视频

94
00:06:59,550 --> 00:07:03,845
I hope this gives you a sense of why we use the cost function we do for
我希望通过本节课您能更好地明白逻辑回归的代价函数

95
00:07:03,845 --> 00:07:05,200
logistic regression.
为什么是那种形式

96
00:07:05,200 --> 00:07:09,287
And with that, I hope you go on to the exercises, the pre exercise and
明白了代价函数的原理 希望您能继续完成前面课程的练习

97
00:07:09,287 --> 00:07:11,277
the quiz questions of this week.
以及本周的测验

98
00:07:11,277 --> 00:07:14,735
And best of luck with both the quizzes, and the following exercise
在课后的小测验和编程练习中 祝您好运

